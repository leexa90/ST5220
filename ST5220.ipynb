{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ST5220",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "dvjPJUy7t6Vp",
        "colab_type": "code",
        "outputId": "c2eea087-da02-400d-afae-f7930a747862",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "cell_type": "code",
      "source": [
        "#install import stuff, you need to login and give google permissions twice\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "!pip install pandas==0.22"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 113597 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n",
            "Requirement already satisfied: pandas==0.22 in /usr/local/lib/python2.7/dist-packages (0.22.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas==0.22) (2018.9)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python2.7/dist-packages (from pandas==0.22) (2.5.3)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from pandas==0.22) (1.14.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil->pandas==0.22) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6jFBslnluMhh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a167a7e9-1cb4-46cb-ced5-35ffc17a8766"
      },
      "cell_type": "code",
      "source": [
        "#mount google drive\n",
        "%cd\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lZZNDOqvuMms",
        "colab_type": "code",
        "outputId": "f5e04488-aee7-4c28-e900-14f3b834e3f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "cell_type": "code",
      "source": [
        "#navigating your google drive and saving files inside.\n",
        "%cd\n",
        "%ls\n",
        "%cd drive/\n",
        "%mkdir github_st5220\n",
        "%cd github_st5220\n",
        "%rm -r ST5220\n",
        "!git clone https://github.com/leexa90/ST5220.git\n",
        "#download model parameters\n",
        "%ls -lh */\n",
        "%ls -lh\n",
        "%cd ST5220\n",
        "%ls -lh"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/\n",
            "/root/drive\n",
            "/root/drive/github_st5220\n",
            "rm: cannot remove 'ST5220': No such file or directory\n",
            "Cloning into 'ST5220'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 8 (delta 0), reused 5 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (8/8), done.\n",
            "Checking out files: 100% (3/3), done.\n",
            "total 62M\n",
            "-rw-r--r-- 1 root root  56M Feb 12 07:43 2015.csv.zip\n",
            "-rw-r--r-- 1 root root 6.7M Feb 12 07:43 codebook15_llcp.pdf\n",
            "-rw-r--r-- 1 root root   75 Feb 12 07:43 README.md\n",
            "total 4.0K\n",
            "drwxr-xr-x 2 root root 4.0K Feb 12 07:40 \u001b[0m\u001b[01;34mST5220\u001b[0m/\n",
            "/root/drive/github_st5220/ST5220\n",
            "total 62M\n",
            "-rw-r--r-- 1 root root  56M Feb 12 07:43 2015.csv.zip\n",
            "-rw-r--r-- 1 root root 6.7M Feb 12 07:43 codebook15_llcp.pdf\n",
            "-rw-r--r-- 1 root root   75 Feb 12 07:43 README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "03jeFWbW5R99",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **0. Figure out data types to efficiently load to memory**"
      ]
    },
    {
      "metadata": {
        "id": "yo5pv5_EuMkv",
        "colab_type": "code",
        "outputId": "6c4b4b6b-1903-4540-db18-ccf4a40395e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import gc\n",
        "from itertools import combinations\n",
        "#small dataset to figure out data types to load them more efficiently to memory\n",
        "data= pd.read_csv('2015.csv.zip', compression='zip',sep=',',nrows=300)\n",
        "dtypes = {}\n",
        "for i in data.keys():\n",
        "    if np.dtype(data[i]) != object:\n",
        "        dtypes[i] = np.float32\n",
        "    else: print i\n",
        "   "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IDATE\n",
            "IMONTH\n",
            "IDAY\n",
            "IYEAR\n",
            "PCDMDECN\n",
            "EXACTOT1\n",
            "EXACTOT2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0ABHkT2Jc9o2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cs0Jr8545cSN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **1. Read entire dataset 400k datapoints, dummy variable for missing values in original dataset= -999**"
      ]
    },
    {
      "metadata": {
        "id": "haNum4tvx7aV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data= pd.read_csv('2015.csv.zip', compression='zip',sep=',',nrows=500000,dtype=dtypes)\n",
        "\n",
        "\n",
        "# Dummy value -999 for missing values\n",
        "monthDay_to_int ={}\n",
        "for i in range(32):\n",
        "    str_i = str(i)\n",
        "    if len(str_i) ==1:\n",
        "        str_i = '0'+str_i\n",
        "    monthDay_to_int[\"b'%s'\" %str_i] = i\n",
        "   \n",
        "# changing datatype of day and month of survey\n",
        "data['IMONTH']=data['IMONTH'].map(monthDay_to_int).astype(np.int32)\n",
        "data['IDAY']=data['IDAY'].map(monthDay_to_int).fillna(15).astype(np.int32)\n",
        "\n",
        "##this finds columns which only have one value\n",
        "data_temp =data.fillna(-999) #fill a temporary datafile with -999 dummy value\n",
        "y = data_temp.describe().iloc[2]!=0 #get cols with std ==0\n",
        "cols = y[y==True].index\n",
        "data = data[list(cols)]\n",
        "gc.collect()\n",
        "\n",
        "# approximate, if there are more than 20 unique values in column, column is quantitive,else qualitiave\n",
        "num_of_unqiue_entries_per_column = data.apply(lambda x : len(pd.unique(x)))\n",
        "quant_var = num_of_unqiue_entries_per_column[num_of_unqiue_entries_per_column>=20].index\n",
        "quant_var = [x for x in quant_var if '_STATE' != x]\n",
        "qual_var = num_of_unqiue_entries_per_column[num_of_unqiue_entries_per_column<20].index\n",
        "data[qual_var] = data[qual_var].fillna(-999).astype(np.int32)\n",
        "# not to fill quant_var because corelation will get affected by dummy var\n",
        "#data[quant_var] = data[quant_var].fillna(-999).astype(np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cLZp0fE-5i4I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **2. Sample 6000 datapoints (data already collected by client)**"
      ]
    },
    {
      "metadata": {
        "id": "kOvRKFmF6LiZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "sample = data.sample(n=6000,replace=True)\n",
        "sample['ori_index'] = sample.index\n",
        "sample  = sample.reset_index(drop=True)\n",
        "data_ori = data.copy(deep=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eAglkeL0ly8L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **3. Cluster questions by correlation matrix and mutual information (normalized) and split them into 15 blocks**"
      ]
    },
    {
      "metadata": {
        "id": "3IRxmVqlc9pv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage \n",
        "import partial_corr #import partial_corr\n",
        "for i in quant_var:\n",
        "    sample[i] = sample[i].fillna(np.mean(sample[i].dropna()))\n",
        "corr = np.abs(sample[quant_var].corr().fillna(0).values)\n",
        "#corr= partial_corr.partial_corr(data[quant_var].fillna(0).values)\n",
        "\n",
        "#corr=data[quant_var].corr() #using population correlation coeficient\n",
        "\n",
        "\n",
        "       \n",
        "   \n",
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "mutual_info = np.zeros((len(qual_var),len(qual_var)))+1\n",
        "for i in range(len(qual_var)):\n",
        "    for j in range(i+1,len(qual_var)):\n",
        "        mutual_info[i,j] = normalized_mutual_info_score(sample[qual_var[i]],sample[qual_var[j]])\n",
        "        mutual_info[j,i] = mutual_info[i,j]\n",
        "   \n",
        "if True:\n",
        "    num_block = 15\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.close()\n",
        "    linked = linkage(corr[:,:], 'single')\n",
        "    dendro  = dendrogram(linked, \n",
        "                orientation='top',\n",
        "                labels=quant_var[:],\n",
        "                distance_sort='descending',\n",
        "                show_leaf_counts=True,color_threshold=15)\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.tight_layout(rect=[0.025, 0.025, .94, .94]);\n",
        "    plt.title('Hierarchical Clustering of\\nQuantitive Variables from abs|correlation matrix|')\n",
        "    plt.ylabel('Distance');plt.xlabel('Quantitive Variables')\n",
        "    plt.savefig('variables_clustering_by_corr_coef.png',dpi=300);plt.close()\n",
        "    a=plt.imshow(corr[np.array(dendro['leaves']),:][:,np.array(dendro['leaves'])]);plt.colorbar(a);plt.close()\n",
        "    df_corr = pd.DataFrame(corr,columns=quant_var)\n",
        "    df_corr.to_csv('corr.csv',index=0)\n",
        "    question_quant = {}\n",
        "    for block in range(num_block):\n",
        "        question_quant[block] = []\n",
        "    for i in range(len(dendro['ivl'])): #give each question by splitting clusters, except state which will be treated differentlys\n",
        "        question_quant[i%num_block] += [dendro['ivl'][i],]\n",
        "    #quanl var\n",
        "    plt.close()\n",
        "    plt.figure(figsize=(10,8))\n",
        "    linked = linkage(mutual_info[:,:], 'single')\n",
        "    dendro  = dendrogram(linked,\n",
        "                orientation='top',\n",
        "                labels=qual_var,\n",
        "                distance_sort='descending',\n",
        "                show_leaf_counts=True,color_threshold=15)\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.tight_layout(rect=[0.025, 0.025, .95, .92]);\n",
        "    plt.title('Hierarchical Clustering of \\ualitative Variables\\n from normalized mutual information matrix',fontsize=24)\n",
        "    plt.ylabel('Distance',fontsize=24);plt.xlabel('Qualitative Variables',fontsize=24)\n",
        "    plt.xticks(range(0,len(dendro['ivl'])*10,10),dendro['ivl'],fontsize=2.5)\n",
        "    plt.savefig('Qual_variables_clustering_by_corr_coef.png',dpi=500);\n",
        "    df_mutual_info = pd.DataFrame(mutual_info,columns=qual_var)\n",
        "    df_mutual_info.to_csv('mutual_info.csv',index=0)\n",
        "    question_qual = {}\n",
        "    for block in range(num_block):\n",
        "        question_qual[block] = []\n",
        "    for i in range(len(dendro['ivl'])):\n",
        "        question_qual[i%num_block] += [dendro['ivl'][i],]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WDaHkL-T5nea",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **4. Defining my own MICE(multiple imputation with chained equations) and KL-divergence formula**"
      ]
    },
    {
      "metadata": {
        "id": "5JssYBtI34G4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def MICE(df,qual_var,quant_var,iterations=10):\n",
        "    from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "    from scipy.stats import mode\n",
        "    # imputes dataframe using descision tree regressor/classifier. \n",
        "    #df =pandas dataframe\n",
        "    #qual_var = categorical var list\n",
        "    #qual_var= qualitative var list\n",
        "    #n iterations\n",
        "    '''NOTE:\n",
        "    MISSING values due to split assigned = -99\n",
        "    Missing values due to interviewee = -999,\n",
        "     filling the -999 with the mode and mean respectively for qual and quant variable '''\n",
        "#if True:\n",
        "#    df = sample60k.copy(deep=True)\n",
        "    missingid = (df==-99)  \n",
        "    for i in qual_var : #ignore warning below, function is just about to be deprecated\n",
        "        df = df.set_value(missingid[missingid[i]==True].index,i,mode(df[df[i]!=-99][i])[0][0])\n",
        "    for i in quant_var :\n",
        "        df = df.set_value(missingid[missingid[i]==True].index,i,np.mean(df[(df[i]!=-99)*(df[i]!=-999)][i]))\n",
        "    df = df.fillna(-999) #some bug in code, this is a work around\n",
        "    df_mean_mode_inpute =df.copy(deep=True)\n",
        "    \n",
        "    for n in range(iterations):\n",
        "        print (n)\n",
        "        for i in quant_var[:] :\n",
        "            model = DecisionTreeRegressor(min_samples_split=30)\n",
        "            feat = [x for x in quant_var if x != i] + list(qual_var)\n",
        "            missing_index = missingid[missingid[i]==True].index\n",
        "            present_index = missingid[missingid[i]==False].index\n",
        "            model.fit(df.iloc[present_index][feat],df.iloc[present_index][i])\n",
        "            missing_predictions = model.predict(df.iloc[missing_index][feat])\n",
        "            df = df.set_value(missing_index,i,missing_predictions) #fill in missing values \n",
        "        for i in qual_var[:] :\n",
        "            model = DecisionTreeClassifier(min_samples_split=30)\n",
        "            feat = [x for x in qual_var if x != i] + list(quant_var)\n",
        "            missing_index = missingid[missingid[i]==True].index\n",
        "            present_index = missingid[missingid[i]==False].index\n",
        "            model.fit(df.iloc[present_index][feat],df.iloc[present_index][i])\n",
        "            missing_predictions = model.predict(df.iloc[missing_index][feat])\n",
        "            df = df.set_value(missing_index,i,missing_predictions)\n",
        "        gc.collect()\n",
        "    print ('finished running with imputed values')\n",
        "    return df,df_mean_mode_inpute\n",
        "\n",
        "def KL_divergence(data1,data2,feat):\n",
        "    a = Counter(data1[feat]) #get discrete discrtirbution in terms of counts\n",
        "    b = Counter(data2[feat])\n",
        "    #normalzie the dsitribution, and fill in empty entries with small dummy number =0.01\n",
        "    for j in np.unique(a.keys()+b.keys()):\n",
        "        if j in a.keys():\n",
        "            a[j] = a[j]*1./len(data1)\n",
        "        else: a[j] = 0.01\n",
        "        if j in b.keys():\n",
        "            b[j] = b[j]*1./len(data2)\n",
        "        else: b[j] = 0.01    \n",
        "    KL_div = 0\n",
        "    #print a,b\n",
        "    for j in np.unique(a.keys()+b.keys()):\n",
        "        KL_div += (a[j])*np.log(a[j]/b[j])\n",
        "    return KL_div "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EU_YaZ1z5tCR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **5. Run experiments across different sample size/question block size and compare the kl-divergence and root-meansquare error (normalized by standard deviation)**\n",
        "\n",
        "dummary variables : \n",
        "-999 are empty responses given \n",
        ",-99 are questions not attempted due to split questionair design\n"
      ]
    },
    {
      "metadata": {
        "id": "CqEBRRhg4Ach",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if True:\n",
        "    data[quant_var] = data[quant_var].fillna(-999).astype(np.float32)\n",
        "    data = data.fillna(-999)\n",
        "    result_MICE= {}\n",
        "    samplesize=6000\n",
        "    np.random.seed(5220)\n",
        "    for samplesize in [6000,12000,25000,50000,100000][:]:\n",
        "        sample60k = data.sample(n=samplesize,replace=True)\n",
        "        sample60k['ori_index'] = sample60k.index\n",
        "        sample60k = sample60k.sort_values('_STATE').reset_index(drop=True) #sort by state, to strafiy sampling by state\n",
        "        Combinations_nC3 = list(combinations(range(num_block),3)) #nC3 combinations of the number of blocks. \n",
        "        dictt_interger_to_combinations = {}\n",
        "        np.random.shuffle(Combinations_nC3)\n",
        "        for i in range(len(Combinations_nC3)):\n",
        "            dictt_interger_to_combinations[i] = Combinations_nC3[i]\n",
        "        sample60k['block'] = (sample60k.index%len(Combinations_nC3))\n",
        "        sample60k['block']=sample60k['block'].map(dictt_interger_to_combinations)\n",
        "        sample60k_ori = sample60k.copy(deep=True)#for validation, 'correct' answer\n",
        "        sample60k[sample60k.keys()[:-2]] = sample60k[sample60k.keys()[:-2]]*0-99 #Initalize datafile with -99, where -99 denotes missing value due to survey split. \n",
        "        sample60k['_STATE'] = sample60k_ori ['_STATE']\n",
        "        sample60k['ori_index'] = sample60k_ori['ori_index']\n",
        "        sample60k['block'] = sample60k_ori ['block']\n",
        "        for blockcomb in Combinations_nC3:\n",
        "            questions_answered = []\n",
        "            for block in blockcomb : #blockcomb is tuple\n",
        "                questions_answered += [x for x in  question_quant[block]]\n",
        "                questions_answered += [x for x in  question_qual[block]]\n",
        "            block_index = sample60k[sample60k['block']==blockcomb].index\n",
        "            sample60k = sample60k.set_value(block_index,\n",
        "                                             questions_answered,\n",
        "                                             sample60k_ori[sample60k_ori['block']==blockcomb][questions_answered])\n",
        "        imputed60k,imputed60k_mean_mode = MICE(df = sample60k.copy(deep=True),qual_var = qual_var,quant_var = quant_var,iterations=10) \n",
        "        quant_result = np.mean((imputed60k[quant_var]-sample60k_ori[quant_var])/np.std(sample60k_ori[quant_var],0)**2,0)\n",
        "        qual_result = map(lambda x : (x,KL_divergence(imputed60k,data,x)), qual_var[:])\n",
        "        result_MICE[(samplesize,num_block)]= (quant_result,qual_result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0g5gfTxSc9qb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 7 gradient based methods (integrated gradients and concept activation)"
      ]
    },
    {
      "metadata": {
        "id": "vqL0M-R6c9qd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "link = 'https://wikimedia.org/api/rest_v1/media/math/render/svg/75fed3df69952839e20426827b30869b7f6515ac'\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML\n",
        "Image(url= './presentation/Integrated_grad_formula.png', width=800, height=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SAWszAw8c9rI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# integrated gradients\n",
        "sess = K.get_session() \n",
        "model.load_weights('mnist.h5')\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "tr_id = 359\n",
        "input_X = X_val[tr_id:tr_id+1]\n",
        "input_X0 = input_X*0 + np.min(input_X)\n",
        "grad = np.zeros(input_X.shape)\n",
        "pred = model.predict(input_X)[0]\n",
        "print map(lambda x : int(x*100),pred)\n",
        "m = 50.\n",
        "vary_pic = np.zeros((28*28,28,28,1))\n",
        "counter = 0\n",
        "for i in range(28):\n",
        "    for j in range(28):\n",
        "        vary_pic[counter,i,j]=1 \n",
        "        counter += 1\n",
        "\n",
        "for i in range(10):\n",
        "    grad = np.zeros(input_X.shape)\n",
        "    if i ==0:\n",
        "        plt.imshow(input_X[0,:,:,0]);plt.show()\n",
        "        plt.show()\n",
        "        f,ax = plt.subplots(2,5,figsize=(18,8))\n",
        "    if True:#i == np.argmax(pred):\n",
        "        outputTensor =model.layers[-2].output[:,i] # (?,)\n",
        "        listOfVariableTensors = model.inputs[0] # (?,28,28,1)\n",
        "        gradients = K.gradients(outputTensor, listOfVariableTensors)\n",
        "        for num in range(50):\n",
        "            grad  += (input_X-input_X0)*sess.run(gradients, feed_dict= {model.input:  input_X0 + (num/m)*(input_X-input_X0)})[0] \n",
        "            if num%50==49:\n",
        "                per = (np.max(grad[0,:,:,0]/m))/(np.max(grad[0,:,:,0]/m)-np.min(grad[0,:,:,0]/m))\n",
        "                cdict1 = {'red':  ((0, 0, 0),   # <- at 0.0, the red component is 0\n",
        "                                   (1-per, 1, 1.0),   # <- at 0.5, the red component is 1\n",
        "                                   (1, 1.0, 1.0)),  # <- at 1.0, the red component is 0\n",
        "\n",
        "                         'green': ((0, 0.0, 0.0),   # <- etc.\n",
        "                                   (1-per, 1.0, 1.0),\n",
        "                                   (1, 0.0, 0.0)),\n",
        "\n",
        "                         'blue':  ((0, 1.0, 1.0),\n",
        "                                   (1-per, 1.0, 1.0),\n",
        "                                   (1, 0, 0))\n",
        "                         }\n",
        "                bwr=LinearSegmentedColormap('bwr', cdict1)\n",
        "                cax = ax[i//5,(i-5)%5].imshow(grad[0,:,:,0]/m,cmap=bwr)\n",
        "                cb = plt.colorbar(cax, cmap=plt.cm.bwr,ax=ax[i//5,(i-5)%5],\n",
        "                          orientation=\"horizontal\", aspect=70) \n",
        "                ax[i//5,(i-5)%5].set_title(str(i)+'_'+str(map(lambda x : int(x*100),pred)[i]))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2lERs0Aic9rZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# concept actiation vectors, concept used is diagonal\n",
        "# MAKING diagonal images\n",
        "import cv2\n",
        "digonal_sample = np.zeros((3000,28,28))\n",
        "input_X0 = input_X[0,:,:,0]*0\n",
        "for i in range(0,28,1):\n",
        "    for j in range(0,28,1):\n",
        "        if i in [j+1,j-1,j,j+2,j-2]: #thickness of two\n",
        "            input_X0[i,j] = 1\n",
        "\n",
        "def make_strip_samples(start,end,input_X0,digonal_sample):\n",
        "    for sample in range(start,end):\n",
        "        leng, = np.random.choice(range(10,20),1)\n",
        "        offsetX, =  np.random.choice(range(28-leng),1)\n",
        "        offsetY, =  np.random.choice(range(28-leng),1)\n",
        "        sub_pic = np.copy(input_X0[:leng,:leng])\n",
        "        digonal_sample[sample,offsetX:offsetX+leng,offsetY:offsetY+leng] = sub_pic\n",
        "    return digonal_sample\n",
        "    \n",
        "digonal_sample = make_strip_samples(0,1000,cv2.GaussianBlur(input_X0,(5,5),0),digonal_sample)\n",
        "\n",
        "\n",
        "input_X0 = input_X[0,:,:,0]*0\n",
        "for i in range(0,28,1):\n",
        "    for j in range(0,28,1):\n",
        "        if i in [j+1,j,j-1]:\n",
        "            input_X0[i,j] = 1\n",
        "digonal_sample = make_strip_samples(1000,2000,cv2.GaussianBlur(input_X0,(5,5),0),digonal_sample)\n",
        "\n",
        "\n",
        "input_X0 = input_X[0,:,:,0]*0\n",
        "for i in range(0,28,1):\n",
        "    for j in range(0,28,1):\n",
        "        if i in [j+1,j-1,j,j+2]:\n",
        "            input_X0[i,j] = 1\n",
        "\n",
        "digonal_sample = make_strip_samples(2000,3000,cv2.GaussianBlur(input_X0,(5,5),0),digonal_sample)\n",
        "digonal_sample = digonal_sample[:,:,::-1]\n",
        "if True:\n",
        "    plt.imshow(digonal_sample[0]);plt.title('sample of diag line');plt.show()\n",
        "    plt.imshow(digonal_sample[1000]);plt.title('sample of diag line');plt.show()\n",
        "    plt.imshow(digonal_sample[2301]);plt.title('sample of diag line');plt.show()\n",
        "digonal_sample = np.expand_dims(digonal_sample,-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w8Q-py5Ec9rq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# USING model.layers[4] output, the directional derivative can be from ANY layer\n",
        "model2 = keras.models.Model(model.input,map(lambda x : x.output,model.layers)[4])\n",
        "model2.summary()\n",
        "pred_diag = np.mean(model2.predict(digonal_sample,batch_size =512//2),(1,2)) #mean poolng of layer from 2D to 1D\n",
        "random_noise = 1.*np.random.randint(0,100,size=X_train[::18].shape)/100\n",
        "pred_nondiag = np.mean(model2.predict(random_noise,batch_size =512//2),(1,2)) #mean poolng of layer from 2D to 1D\n",
        "pred_X = np.concatenate([digonal_sample,random_noise],0)\n",
        "print pred_diag.shape,pred_nondiag.shape,pred_X.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "azOmfzcoc9r3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "sess = K.get_session() \n",
        "model.load_weights('mnist.h5')\n",
        "\n",
        "if True:\n",
        "    # train SVM from outputs from model layer\n",
        "    pred_diag_unwrap = pred_diag\n",
        "    pred_nondiag_unwrap = pred_nondiag\n",
        "    pred_comb = np.concatenate([pred_diag_unwrap,pred_nondiag_unwrap],0)\n",
        "    labels = np.array([1,]*3000 + [0,]*3056) \n",
        "    svm_model = LinearSVC().fit(pred_comb,labels)\n",
        "    preds = svm_model.predict(pred_comb)\n",
        "    print 'svm confusion matrix:\\n',confusion_matrix(labels,preds)\n",
        "    support_vec = svm_model.coef_\n",
        "    for output in range(10):\n",
        "        outputTensor =model.layers[-2].output[:,output] # (?,)\n",
        "        listOfVariableTensors = map(lambda x : x.output,model.layers)[4]# (?,28,28,1)\n",
        "        gradients = K.gradients(outputTensor, listOfVariableTensors)\n",
        "        grad  = sess.run(gradients, feed_dict= {model.input:  X_train[:600]})[0][:,:,:,:] # 600,14,14,64\n",
        "        grad = np.mean(grad,(1,2)) #mean pooling of gradients from 2D to 1D, # 600，1,1,64\n",
        "        grad = np.reshape(grad,(600,64)) #reshape gradient to 64 vector\n",
        "        grad2 = np.matmul(grad,svm_model.coef_.T/np.sum(svm_model.coef_.T**2)**.5) # get directional derivative\n",
        "        print 'Mean directional derivative of mnist at logit :',output\n",
        "        print np.round(np.mean(grad2),2)\n",
        "        val2 = np.matmul(grad,svm_model.coef_.T/np.sum(svm_model.coef_.T**2)**.5)\n",
        "        plt.plot(output,np.mean(grad2),'ro')\n",
        "        plt.text(output+0.1,np.mean(grad2)+0.02,str(output),fontsize=30)\n",
        "\n",
        "plt.ylabel('avg directional derivative',fontsize=30)\n",
        "plt.xlabel('digit',fontsize=30)\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}